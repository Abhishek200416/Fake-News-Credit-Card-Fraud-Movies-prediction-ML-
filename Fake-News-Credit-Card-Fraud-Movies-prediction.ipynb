{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "58291cf4",
   "metadata": {},
   "source": [
    "# **Fake-News-Credit-Card-Fraud-Movies-prediction**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b656a30b",
   "metadata": {},
   "source": [
    "**Datasets:**\n",
    "\n",
    "- Fake News: https://www.kaggle.com/datasets/emineyetm/fake-news-detection-datasets\n",
    "- Credit Card Fraud: https://www.kaggle.com/datasets/mlg-ulb/creditcardfraud\n",
    "- Movies: https://www.kaggle.com/datasets/parasharmanas/movie-recommendation-system\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a129472e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os, pandas as pd, numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "def SAFE_READ_CSV(preferred_paths, fallback_msg):\n",
    "    for p in preferred_paths:\n",
    "        if os.path.exists(p):\n",
    "            try:\n",
    "                try:\n",
    "                    df = pd.read_csv(p)\n",
    "                except UnicodeDecodeError:\n",
    "                    df = pd.read_csv(p, encoding='latin-1')\n",
    "                print(f\"Loaded dataset from: {p}\")\n",
    "                return df\n",
    "            except Exception as e:\n",
    "                print(f\"Found {p} but couldn't read as CSV: {e}\")\n",
    "    print(fallback_msg)\n",
    "    manual = input('‚û° Enter full path to your CSV (or press Enter to cancel): ').strip()\n",
    "    if manual:\n",
    "        if not os.path.exists(manual):\n",
    "            raise FileNotFoundError(f'Path does not exist: {manual}')\n",
    "        try:\n",
    "            return pd.read_csv(manual)\n",
    "        except UnicodeDecodeError:\n",
    "            return pd.read_csv(manual, encoding='latin-1')\n",
    "    raise FileNotFoundError('CSV not found. Place the file next to this notebook or give a valid path.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "456999e6",
   "metadata": {},
   "source": [
    "## Fake News Detection (TF‚ÄëIDF + Logistic Regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "54fcd46c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded dataset from: data/News/True.csv\n",
      "Loaded dataset from: data/News/Fake.csv\n",
      "Accuracy: 0.9868596881959911\n",
      "ROC-AUC: 0.999084486151076\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "        real       0.98      0.99      0.99      4284\n",
      "        fake       0.99      0.98      0.99      4696\n",
      "\n",
      "    accuracy                           0.99      8980\n",
      "   macro avg       0.99      0.99      0.99      8980\n",
      "weighted avg       0.99      0.99      0.99      8980\n",
      "\n",
      "Confusion Matrix:\n",
      " [[4247   37]\n",
      " [  81 4615]]\n",
      "Saved ‚Üí fake_news_tfidf_lr.joblib\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, classification_report, confusion_matrix\n",
    "import joblib, numpy as np\n",
    "\n",
    "df_true = SAFE_READ_CSV(['data/News/True.csv','True.csv','/mnt/data/True.csv'], \"Place 'True.csv' in data/ or provide a path\")\n",
    "df_fake = SAFE_READ_CSV(['data/News/Fake.csv','Fake.csv','/mnt/data/Fake.csv'], \"Place 'Fake.csv' in data/ or provide a path\")\n",
    "\n",
    "\n",
    "def unify(df):\n",
    "    if 'text' in df.columns:\n",
    "        return df[['text']].copy()\n",
    "    candidates = [c for c in df.columns if str(c).lower() in ['title','subject','content','article','body']]\n",
    "    if candidates:\n",
    "        return df[candidates].astype(str).agg(' '.join, axis=1).to_frame('text')\n",
    "    str_cols = [c for c in df.columns if df[c].dtype == 'object']\n",
    "    if str_cols:\n",
    "        lengths = df[str_cols].astype(str).apply(lambda s: s.str.len().fillna(0).mean())\n",
    "        best = lengths.sort_values(ascending=False).index[0]\n",
    "        return df[[best]].rename(columns={best: 'text'})\n",
    "    raise ValueError('No text columns detected')\n",
    "\n",
    "X_true = unify(df_true); X_true['label'] = 0\n",
    "X_fake = unify(df_fake); X_fake['label'] = 1\n",
    "df_news = pd.concat([X_true, X_fake], ignore_index=True).dropna(subset=['text'])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_news['text'], df_news['label'], test_size=0.2, random_state=42, stratify=df_news['label'])\n",
    "\n",
    "pipe = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer(stop_words='english', ngram_range=(1,2), min_df=3, max_df=0.9)),\n",
    "    ('clf', LogisticRegression(max_iter=2000, class_weight='balanced'))\n",
    "])\n",
    "pipe.fit(X_train, y_train)\n",
    "pred = pipe.predict(X_test)\n",
    "proba = pipe.predict_proba(X_test)[:,1]\n",
    "print('Accuracy:', accuracy_score(y_test, pred))\n",
    "print('ROC-AUC:', roc_auc_score(y_test, proba))\n",
    "print('\\nClassification Report:\\n', classification_report(y_test, pred, target_names=['real','fake']))\n",
    "print('Confusion Matrix:\\n', confusion_matrix(y_test, pred))\n",
    "joblib.dump(pipe, 'fake_news_tfidf_lr.joblib')\n",
    "print('Saved ‚Üí fake_news_tfidf_lr.joblib')\n",
    "\n",
    "def explain_text(txt, pipeline, top_k=8):\n",
    "    vec = pipeline.named_steps['tfidf']\n",
    "    clf = pipeline.named_steps['clf']\n",
    "    Xv = vec.transform([txt])\n",
    "    coefs = clf.coef_.ravel()\n",
    "    feats = np.array(vec.get_feature_names_out())\n",
    "    vals = Xv.toarray().ravel()\n",
    "    contrib = vals * coefs\n",
    "    pos = np.argsort(contrib)[-top_k:][::-1]\n",
    "    neg = np.argsort(contrib)[:top_k]\n",
    "    return list(zip(feats[pos], contrib[pos])), list(zip(feats[neg], contrib[neg]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2f593d1",
   "metadata": {},
   "source": [
    " ensure your trained model file exists in the same folder:\n",
    "   fake_news_tfidf_lr.joblib\n",
    "\n",
    "pip install -U streamlit scikit-learn joblib pandas numpy beautifulsoup4 lxml requests\n",
    "\n",
    "\n",
    "pip install newspaper3k\n",
    "\n",
    "streamlit\n",
    "scikit-learn\n",
    "joblib\n",
    "pandas\n",
    "numpy\n",
    "beautifulsoup4\n",
    "lxml\n",
    "requests\n",
    "newspaper3k   \n",
    "\n",
    "streamlit run app_fake_news_streamlit.py\n",
    "requriements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcac6304",
   "metadata": {},
   "outputs": [],
   "source": [
    "# app_fake_news_streamlit.py\n",
    "# Fake News Detector ‚Äî TF-IDF(1‚Äì2) + Logistic Regression\n",
    "# ‚Ä¢ Paste text or URL (robots.txt-aware smart scraping; prefers newspaper3k)\n",
    "# ‚Ä¢ Token-level explainability (top FAKE vs REAL) + bar charts + inline highlights\n",
    "# ‚Ä¢ Batch CSV scoring + download\n",
    "# ‚Ä¢ Decision threshold slider, quality-gate, caching, resilient errors\n",
    "# ‚Ä¢ Polished UI with badges, metrics, and JSON payload\n",
    "\n",
    "import os, re, time, json, joblib, numpy as np, pandas as pd, streamlit as st\n",
    "from urllib.parse import urlparse\n",
    "from urllib.robotparser import RobotFileParser\n",
    "\n",
    "# =============== Streamlit Config & Style ===============\n",
    "st.set_page_config(page_title=\"Fake News Detector\", page_icon=\"üì∞\", layout=\"wide\")\n",
    "\n",
    "CUSTOM_CSS = \"\"\"\n",
    "<style>\n",
    ":root { --pill:#eef3ff; --accent:#4c78ff; --good:#10b981; --bad:#ef4444; }\n",
    ".block-container { padding-top: 1.0rem; }\n",
    ".pill { display:inline-block; padding:.25rem .65rem; border-radius:999px; background:var(--pill); }\n",
    ".badge-real { background:#ecfdf5; color:#065f46; border:1px solid #10b98133; }\n",
    ".badge-fake { background:#fef2f2; color:#7f1d1d; border:1px solid #ef444433; }\n",
    ".small { opacity:.75; font-size:.90rem; }\n",
    ".token-hi { padding:.05rem .25rem; border-radius:.25rem; }\n",
    ".token-hi.real { background:#ecfdf5; }\n",
    ".token-hi.fake { background:#fef2f2; }\n",
    "footer { visibility: hidden; }  /* tidy the footer */\n",
    "</style>\n",
    "\"\"\"\n",
    "st.markdown(CUSTOM_CSS, unsafe_allow_html=True)\n",
    "\n",
    "st.title(\"üì∞ Fake News Detector\")\n",
    "st.caption(\"TF-IDF + Logistic Regression ‚Ä¢ URL scrape or paste text ‚Ä¢ Token contributions\")\n",
    "\n",
    "# =============== Load trained pipeline (cached) ===============\n",
    "@st.cache_resource\n",
    "def load_pipeline(model_path=\"fake_news_tfidf_lr.joblib\"):\n",
    "    \"\"\"\n",
    "    Cache the trained scikit-learn Pipeline for fast, repeatable inference.\n",
    "    Expects a Pipeline with steps 'tfidf' (TfidfVectorizer) and 'clf' (LogisticRegression).\n",
    "    \"\"\"\n",
    "    if not os.path.exists(model_path):\n",
    "        raise FileNotFoundError(\n",
    "            f\"Model file '{model_path}' not found. Train your notebook to create it.\"\n",
    "        )\n",
    "    pipe = joblib.load(model_path)\n",
    "    if \"tfidf\" not in pipe.named_steps or \"clf\" not in pipe.named_steps:\n",
    "        raise ValueError(\"Expected Pipeline with steps: 'tfidf' and 'clf'.\")\n",
    "    return pipe\n",
    "\n",
    "pipe = load_pipeline()\n",
    "vec = pipe.named_steps[\"tfidf\"]\n",
    "clf = pipe.named_steps[\"clf\"]\n",
    "\n",
    "# =============== Utilities ===============\n",
    "def is_url(s: str) -> bool:\n",
    "    try:\n",
    "        p = urlparse(s.strip())\n",
    "        return bool(p.scheme and p.netloc)\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "@st.cache_data(ttl=180)\n",
    "def robots_allowed(url: str, ua: str = \"Mozilla/5.0\") -> bool:\n",
    "    \"\"\"robots.txt check; return True if allowed or robots missing.\"\"\"\n",
    "    try:\n",
    "        p = urlparse(url)\n",
    "        robots_url = f\"{p.scheme}://{p.netloc}/robots.txt\"\n",
    "        rp = RobotFileParser()\n",
    "        rp.set_url(robots_url)\n",
    "        rp.read()\n",
    "        return rp.can_fetch(ua, url) if rp.default_entry is not None else True\n",
    "    except Exception:\n",
    "        return True\n",
    "\n",
    "def _scrape_bs4(url: str, timeout: int = 15) -> str:\n",
    "    import requests\n",
    "    from bs4 import BeautifulSoup\n",
    "    headers = {\n",
    "        \"User-Agent\": (\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) \"\n",
    "                       \"AppleWebKit/537.36 (KHTML, like Gecko) Chrome/124 Safari/537.36\")\n",
    "    }\n",
    "    r = requests.get(url, headers=headers, timeout=timeout)\n",
    "    r.raise_for_status()\n",
    "    soup = BeautifulSoup(r.text, \"lxml\")\n",
    "    article = soup.find(\"article\")\n",
    "    ps = (article.find_all(\"p\") if article else soup.find_all(\"p\"))\n",
    "    text = \" \".join(p.get_text(\" \", strip=True) for p in ps if p.get_text(strip=True))\n",
    "    return text.strip()\n",
    "\n",
    "def _scrape_newspaper3k(url: str, timeout: int = 20) -> str:\n",
    "    from newspaper import Article\n",
    "    art = Article(url, keep_article_html=False, language=\"en\")\n",
    "    art.download()\n",
    "    t0 = time.time()\n",
    "    while art.download_state == 0 and time.time() - t0 < timeout:\n",
    "        time.sleep(0.1)\n",
    "    art.parse()\n",
    "    title = (art.title or \"\").strip()\n",
    "    text  = (art.text  or \"\").strip()\n",
    "    return \" \".join([title, text]).strip()\n",
    "\n",
    "@st.cache_data(ttl=300)\n",
    "def smart_scrape(url: str) -> str:\n",
    "    \"\"\"Prefer newspaper3k; fallback to BeautifulSoup; normalize + clamp length.\"\"\"\n",
    "    if not robots_allowed(url):\n",
    "        return \"[SCRAPE_BLOCKED] robots.txt disallows fetching this URL.\"\n",
    "    try:\n",
    "        import newspaper  # noqa: F401\n",
    "        txt = _scrape_newspaper3k(url)\n",
    "    except Exception:\n",
    "        try:\n",
    "            txt = _scrape_bs4(url)\n",
    "        except Exception as e:\n",
    "            return f\"[SCRAPE_ERROR] {e}\"\n",
    "    txt = re.sub(r\"\\s+\", \" \", txt).strip()\n",
    "    return txt[:25000] if len(txt) > 25000 else txt\n",
    "\n",
    "def explain_text(txt: str, top_k: int = 10):\n",
    "    \"\"\"\n",
    "    Token contributions:\n",
    "      ‚Ä¢ top_fake: pushes toward FAKE (positive coef)\n",
    "      ‚Ä¢ top_real: pushes toward REAL (negative coef)\n",
    "    \"\"\"\n",
    "    Xv = vec.transform([txt])\n",
    "    coefs = clf.coef_.ravel()\n",
    "    feats = np.array(vec.get_feature_names_out())\n",
    "    vals = Xv.toarray().ravel()\n",
    "    contrib = vals * coefs\n",
    "    pos_idx = np.argsort(contrib)[-top_k:][::-1]\n",
    "    neg_idx = np.argsort(contrib)[:top_k]\n",
    "    top_fake = [(feats[i], float(contrib[i])) for i in pos_idx if vals[i] > 0]\n",
    "    top_real = [(feats[i], float(contrib[i])) for i in neg_idx if vals[i] > 0]\n",
    "    return top_fake, top_real\n",
    "\n",
    "def score_text(text: str, threshold: float = 0.5):\n",
    "    prob_fake = float(pipe.predict_proba([text])[0][1])\n",
    "    pred = int(prob_fake >= threshold)\n",
    "    return (\"FAKE\" if pred == 1 else \"REAL\"), prob_fake\n",
    "\n",
    "def highlight_tokens(text: str, top_fake, top_real):\n",
    "    \"\"\"Inline highlight of top tokens (exact word match, case-insensitive).\"\"\"\n",
    "    if not text or (not top_fake and not top_real):\n",
    "        return text\n",
    "    tokens = [t for t,_ in (top_fake + top_real)]\n",
    "    tokens = sorted(set(tokens), key=len, reverse=True)[:60]\n",
    "    if not tokens: return text\n",
    "    label_map = {t.lower():\"fake\" for t,_ in top_fake}\n",
    "    label_map.update({t.lower():\"real\" for t,_ in top_real})\n",
    "    def repl(m):\n",
    "        w = m.group(0)\n",
    "        cls = label_map.get(w.lower(), \"real\")\n",
    "        return f'<span class=\"token-hi {cls}\">{w}</span>'\n",
    "    pattern = r\"\\b(\" + \"|\".join(re.escape(t) for t in tokens) + r\")\\b\"\n",
    "    return re.sub(pattern, repl, text, flags=re.IGNORECASE)\n",
    "\n",
    "# =============== Sidebar (Controls) ===============\n",
    "with st.sidebar:\n",
    "    st.header(\"‚öôÔ∏è Settings\")\n",
    "    threshold = st.slider(\"Decision threshold (FAKE if prob ‚â• threshold)\", 0.10, 0.90, 0.50, 0.01)\n",
    "    top_k = st.slider(\"Tokens to show per side\", 5, 25, 10, 1)\n",
    "    st.markdown(\"**Model file:** `fake_news_tfidf_lr.joblib`\")\n",
    "    st.markdown(\"**Dataset:** Fake/True News (two CSVs). See Kaggle dataset page.\")\n",
    "    st.markdown('<span class=\"small\">Model: TF-IDF (1‚Äì2 grams, min_df=3, max_df=0.9) + Logistic Regression (class_weight=\\\"balanced\\\").</span>',\n",
    "                unsafe_allow_html=True)\n",
    "\n",
    "# =============== Tabs ===============\n",
    "tab1, tab2 = st.tabs([\"Single Article\", \"Batch CSV (optional)\"])\n",
    "\n",
    "# ---------- Tab 1: Single Article ----------\n",
    "with tab1:\n",
    "    st.subheader(\"Single Article\")\n",
    "    src = st.text_area(\"Paste a news article, social post, blog text, or a URL\",\n",
    "                       height=180, placeholder=\"Paste text or https://example.com/article\")\n",
    "\n",
    "    c1, c2, c3 = st.columns([1,1,1])\n",
    "    with c1:\n",
    "        min_chars = st.number_input(\"Minimum characters (quality gate)\", 0, 1000, 40, 10)\n",
    "    with c2:\n",
    "        show_bars = st.checkbox(\"Show contribution bar charts\", True)\n",
    "    with c3:\n",
    "        show_inline = st.checkbox(\"Inline token highlights\", True)\n",
    "\n",
    "    analyze = st.button(\"Analyze\", type=\"primary\", use_container_width=True)\n",
    "\n",
    "    if analyze and src.strip():\n",
    "        text = src.strip()\n",
    "        if is_url(text):\n",
    "            with st.spinner(\"Scraping article‚Ä¶\"):\n",
    "                text = smart_scrape(text)\n",
    "            if text.startswith(\"[SCRAPE_BLOCKED]\"):\n",
    "                st.error(text); st.stop()\n",
    "            if text.startswith(\"[SCRAPE_ERROR]\"):\n",
    "                st.error(text); st.stop()\n",
    "\n",
    "        if len(text) < min_chars:\n",
    "            st.warning(\"Input looks too short for reliable classification. Add more text.\")\n",
    "\n",
    "        label, prob_fake = score_text(text, threshold=threshold)\n",
    "\n",
    "        # KPI badges\n",
    "        cL, cR = st.columns([1,1])\n",
    "        with cL:\n",
    "            badge = 'badge-fake\">FAKE' if label == \"FAKE\" else 'badge-real\">REAL'\n",
    "            st.markdown(f'### Prediction: <span class=\"pill {badge}</span>', unsafe_allow_html=True)\n",
    "        with cR:\n",
    "            st.metric(label=\"Probability (FAKE)\", value=f\"{prob_fake:.3f}\",\n",
    "                      delta=f\"{(prob_fake - threshold):+.3f} vs threshold\")\n",
    "\n",
    "        # Token contributions\n",
    "        top_fake, top_real = explain_text(text, top_k=top_k)\n",
    "\n",
    "        colA, colB = st.columns(2)\n",
    "        with colA:\n",
    "            st.subheader(\"Tokens pushing ‚ûú FAKE\")\n",
    "            df_fake = pd.DataFrame(top_fake, columns=[\"token\", \"contribution\"])\n",
    "            st.dataframe(df_fake, use_container_width=True, hide_index=True)\n",
    "            if show_bars and not df_fake.empty:\n",
    "                st.bar_chart(df_fake.set_index(\"token\"))\n",
    "        with colB:\n",
    "            st.subheader(\"Tokens pushing ‚ûú REAL\")\n",
    "            df_real = pd.DataFrame(top_real, columns=[\"token\", \"contribution\"])\n",
    "            st.dataframe(df_real, use_container_width=True, hide_index=True)\n",
    "            if show_bars and not df_real.empty:\n",
    "                st.bar_chart(df_real.assign(contribution=lambda d: -d[\"contribution\"]).set_index(\"token\"))\n",
    "\n",
    "        # Inline highlight preview + JSON\n",
    "        with st.expander(\"Show analyzed text\"):\n",
    "            if show_inline and (top_fake or top_real):\n",
    "                st.markdown(highlight_tokens(text, top_fake, top_real), unsafe_allow_html=True)\n",
    "            else:\n",
    "                st.write(text)\n",
    "        with st.expander(\"Prediction JSON\"):\n",
    "            payload = {\n",
    "                \"label\": label,\n",
    "                \"prob_fake\": round(prob_fake, 6),\n",
    "                \"prob_real\": round(1 - prob_fake, 6),\n",
    "                \"threshold\": threshold,\n",
    "                \"top_tokens_fake\": top_fake,\n",
    "                \"top_tokens_real\": top_real\n",
    "            }\n",
    "            st.code(json.dumps(payload, indent=2))\n",
    "\n",
    "# ---------- Tab 2: Batch CSV ----------\n",
    "with tab2:\n",
    "    st.subheader(\"Batch CSV Scoring\")\n",
    "    st.write(\"Upload a CSV with a text column (e.g., `text`, `content`, `article`, `body`, or `message`).\")\n",
    "    up = st.file_uploader(\"CSV file\", type=[\"csv\"])\n",
    "    if up:\n",
    "        df_in = pd.read_csv(up)\n",
    "        st.dataframe(df_in.head(), use_container_width=True)\n",
    "        guess = None\n",
    "        for c in df_in.columns:\n",
    "            if str(c).strip().lower() in [\"text\", \"content\", \"article\", \"body\", \"message\"]:\n",
    "                guess = c; break\n",
    "        text_col = st.selectbox(\"Text column\", options=df_in.columns.tolist(),\n",
    "                                index=df_in.columns.get_loc(guess) if guess in df_in.columns else 0)\n",
    "\n",
    "        if st.button(\"Score CSV\", use_container_width=True):\n",
    "            with st.spinner(\"Scoring‚Ä¶\"):\n",
    "                series = df_in[text_col].astype(str).fillna(\"\")\n",
    "                Xv = vec.transform(series)\n",
    "                proba_fake = pipe.predict_proba(Xv)[:, 1]\n",
    "                pred = (proba_fake >= threshold).astype(int)\n",
    "                df_out = df_in.copy()\n",
    "                df_out[\"prob_fake\"] = proba_fake\n",
    "                df_out[\"prediction\"] = pred\n",
    "            st.success(\"Scoring complete.\")\n",
    "            st.dataframe(df_out.head(20), use_container_width=True)\n",
    "            st.download_button(\n",
    "                \"Download results (CSV)\",\n",
    "                df_out.to_csv(index=False).encode(\"utf-8\"),\n",
    "                file_name=\"fake_news_scored.csv\",\n",
    "                mime=\"text/csv\",\n",
    "                use_container_width=True\n",
    "            )\n",
    "\n",
    "# =============== Footer ===============\n",
    "st.markdown(\"---\")\n",
    "st.markdown(\n",
    "    \"\"\"\n",
    "**Notes**\n",
    "- Model: TF-IDF (1‚Äì2 grams) + Logistic Regression trained on the Fake/True news CSVs.\n",
    "- Outputs are probabilistic and context-dependent; always consider source, date and domain knowledge.\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a4f1a1c",
   "metadata": {},
   "source": [
    "## Credit Card Fraud Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "71e855da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded dataset from: data/cerditcard/creditcard.csv\n",
      "LR ROC-AUC: 0.972083 | PR-AUC (AP): 0.718971 | thr*: 1.000000\n",
      "\n",
      "Classification Report (LR):\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9997    0.9997    0.9997     56864\n",
      "           1     0.8247    0.8163    0.8205        98\n",
      "\n",
      "    accuracy                         0.9994     56962\n",
      "   macro avg     0.9122    0.9080    0.9101     56962\n",
      "weighted avg     0.9994    0.9994    0.9994     56962\n",
      "\n",
      "Confusion Matrix (LR):\n",
      " [[56847    17]\n",
      " [   18    80]]\n",
      "Saved ‚Üí fraud_lr_balanced.joblib\n",
      "\n",
      "PCA-Anomaly PR-AUC (AP): 0.656308 | thr*: 0.034632\n",
      "Classification Report (PCA):\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9997    0.9993    0.9995     56864\n",
      "           1     0.6838    0.8163    0.7442        98\n",
      "\n",
      "    accuracy                         0.9990     56962\n",
      "   macro avg     0.8417    0.9078    0.8719     56962\n",
      "weighted avg     0.9991    0.9990    0.9991     56962\n",
      "\n",
      "Confusion Matrix (PCA):\n",
      " [[56827    37]\n",
      " [   18    80]]\n",
      "Saved ‚Üí fraud_pca_anomaly.joblib\n",
      "\n",
      "--- Fusion=OR report (higher recall) ---\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9997    0.9993    0.9995     56864\n",
      "           1     0.6723    0.8163    0.7373        98\n",
      "\n",
      "    accuracy                         0.9990     56962\n",
      "   macro avg     0.8360    0.9078    0.8684     56962\n",
      "weighted avg     0.9991    0.9990    0.9990     56962\n",
      "\n",
      "\n",
      "--- Fusion=AND report (higher precision) ---\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9997    0.9997    0.9997     56864\n",
      "           1     0.8421    0.8163    0.8290        98\n",
      "\n",
      "    accuracy                         0.9994     56962\n",
      "   macro avg     0.9209    0.9080    0.9144     56962\n",
      "weighted avg     0.9994    0.9994    0.9994     56962\n",
      "\n",
      "\n",
      "--- Fusion=AVG report (w=0.7) ---\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9998    0.9892    0.9945     56864\n",
      "           1     0.1264    0.9082    0.2219        98\n",
      "\n",
      "    accuracy                         0.9890     56962\n",
      "   macro avg     0.5631    0.9487    0.6082     56962\n",
      "weighted avg     0.9983    0.9890    0.9932     56962\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\abhis\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py:493: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# ==== Credit Card Fraud Detection (Upgraded) ===================================\n",
    "# Supervised: Logistic Regression (balanced) with PR-optimal threshold\n",
    "# Anomaly:    PCA reconstruction error (no neighbors/IF imports)\n",
    "# Fusion:     AND / OR / weighted-average of LR and PCA scores\n",
    "# Saves:\n",
    "#   - fraud_lr_balanced.joblib\n",
    "#   - fraud_pca_anomaly.joblib\n",
    "#   - (helpers at bottom to score single/batch transactions consistently)\n",
    "# ===============================================================================\n",
    "import os, numpy as np, pandas as pd, joblib\n",
    "from dataclasses import dataclass\n",
    "from typing import Tuple, Literal, Optional, Dict\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import (\n",
    "    classification_report, confusion_matrix, roc_auc_score,\n",
    "    precision_recall_curve, average_precision_score\n",
    ")\n",
    "\n",
    "# ---------------- SAFE_READ_CSV ----------------\n",
    "try:\n",
    "    SAFE_READ_CSV\n",
    "except NameError:\n",
    "    def SAFE_READ_CSV(preferred_paths, fallback_msg):\n",
    "        for p in preferred_paths:\n",
    "            if os.path.exists(p):\n",
    "                try:\n",
    "                    return pd.read_csv(p)\n",
    "                except Exception as e:\n",
    "                    print(f\"Found {p} but couldn't read it: {e}\")\n",
    "        raise FileNotFoundError(fallback_msg)\n",
    "\n",
    "# ---------------- Load data ----------------\n",
    "paths = [\n",
    "    'data/cerditcard/creditcard.csv',   # your existing path\n",
    "    'data/creditcard/creditcard.csv',   # common corrected path\n",
    "    'creditcard.csv', '/mnt/data/creditcard.csv'\n",
    "]\n",
    "df_cc = SAFE_READ_CSV(paths, \"Place 'creditcard.csv' in data/ or provide a valid path\")\n",
    "assert 'Class' in df_cc.columns, \"Expected binary target 'Class' (0=legit,1=fraud).\"\n",
    "\n",
    "X = df_cc.drop(columns=['Class'])\n",
    "y = df_cc['Class'].astype(int)\n",
    "\n",
    "# Optional: common tweaks\n",
    "# if 'Time' in X.columns:   X = X.drop(columns=['Time'])\n",
    "# if 'Amount' in X.columns: X = X.assign(Amount=np.log1p(X['Amount']))\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.20, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# ---------------- Helpers ----------------\n",
    "def pr_optimal_threshold(y_true: np.ndarray, scores: np.ndarray) -> float:\n",
    "    \"\"\"Choose threshold that maximizes F1 on the PR curve.\"\"\"\n",
    "    prec, rec, thr = precision_recall_curve(y_true, scores)\n",
    "    f1 = 2 * prec * rec / (prec + rec + 1e-9)\n",
    "    best = np.nanargmax(f1)\n",
    "    return float(thr[best-1] if best > 0 and (best-1) < len(thr) else 0.5)\n",
    "\n",
    "# ---------------- 1) Logistic Regression (balanced) ----------------\n",
    "# If you want probability calibration, set CALIBRATE=True (kept off to be fast & simple)\n",
    "CALIBRATE = False\n",
    "\n",
    "pipe_lr = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('clf', LogisticRegression(max_iter=5000, class_weight='balanced', solver='lbfgs'))\n",
    "])\n",
    "pipe_lr.fit(X_train, y_train)\n",
    "\n",
    "proba_lr = pipe_lr.predict_proba(X_test)[:, 1]\n",
    "thr_lr = pr_optimal_threshold(y_test.values, proba_lr)\n",
    "pred_lr = (proba_lr >= thr_lr).astype(int)\n",
    "\n",
    "roc_auc = roc_auc_score(y_test, proba_lr)\n",
    "ap_lr = average_precision_score(y_test, proba_lr)\n",
    "\n",
    "print(f'LR ROC-AUC: {roc_auc:.6f} | PR-AUC (AP): {ap_lr:.6f} | thr*: {thr_lr:.6f}')\n",
    "print('\\nClassification Report (LR):\\n', classification_report(y_test, pred_lr, digits=4))\n",
    "print('Confusion Matrix (LR):\\n', confusion_matrix(y_test, pred_lr))\n",
    "\n",
    "# Save model with threshold and feature order to avoid drift later\n",
    "lr_bundle = {\n",
    "    'model': pipe_lr,\n",
    "    'thr': thr_lr,\n",
    "    'feature_names': list(X.columns)\n",
    "}\n",
    "joblib.dump(lr_bundle, 'fraud_lr_balanced.joblib')\n",
    "print('Saved ‚Üí fraud_lr_balanced.joblib')\n",
    "\n",
    "# ---------------- 2) PCA anomaly detector (reconstruction error) ----------------\n",
    "def fit_pca_anomaly(X_train_df: pd.DataFrame, y_train_series: pd.Series, var_keep: float = 0.95) -> Dict:\n",
    "    \"\"\"Fit PCA on normal class (y==0) with RobustScaler. Return components & scaler stats.\"\"\"\n",
    "    Xn = X_train_df[y_train_series == 0].values.astype(float)\n",
    "\n",
    "    scaler = RobustScaler()\n",
    "    Xn_rb = scaler.fit_transform(Xn)\n",
    "\n",
    "    # SVD: X ‚âà U S V^T ; keep k s.t. cumulative variance ‚â• var_keep\n",
    "    U, S, Vt = np.linalg.svd(Xn_rb, full_matrices=False)\n",
    "    var = (S ** 2)\n",
    "    cum = np.cumsum(var) / np.sum(var)\n",
    "    k = int(np.searchsorted(cum, var_keep) + 1)\n",
    "    Vt_k = Vt[:k, :]  # k x d\n",
    "\n",
    "    return {\n",
    "        'center_': scaler.center_,\n",
    "        'scale_': scaler.scale_,\n",
    "        'Vt_k': Vt_k,\n",
    "        'k': k,\n",
    "        'var_keep': var_keep,\n",
    "        'feature_names': list(X_train_df.columns)\n",
    "    }\n",
    "\n",
    "def pca_recon_error(bundle: Dict, X_df: pd.DataFrame) -> np.ndarray:\n",
    "    \"\"\"Compute reconstruction error with saved components/scaler.\"\"\"\n",
    "    # enforce feature order\n",
    "    X_df = X_df.reindex(columns=bundle['feature_names'], fill_value=0)\n",
    "    Xv = X_df.values.astype(float)\n",
    "\n",
    "    X_rb = (Xv - bundle['center_']) / (bundle['scale_'] + 1e-12)\n",
    "    Vt_k = bundle['Vt_k']\n",
    "    Z = X_rb @ Vt_k.T\n",
    "    X_rec = Z @ Vt_k\n",
    "    err = np.sum((X_rb - X_rec) ** 2, axis=1)\n",
    "    return err\n",
    "\n",
    "pca_bundle = fit_pca_anomaly(X_train, y_train, var_keep=0.95)\n",
    "err_test = pca_recon_error(pca_bundle, X_test)\n",
    "# min-max normalize to [0,1] as fraudiness score\n",
    "score_pca = (err_test - err_test.min()) / (err_test.max() - err_test.min() + 1e-12)\n",
    "thr_pca = pr_optimal_threshold(y_test.values, score_pca)\n",
    "pred_pca = (score_pca >= thr_pca).astype(int)\n",
    "ap_pca = average_precision_score(y_test, score_pca)\n",
    "\n",
    "print(f'\\nPCA-Anomaly PR-AUC (AP): {ap_pca:.6f} | thr*: {thr_pca:.6f}')\n",
    "print('Classification Report (PCA):\\n', classification_report(y_test, pred_pca, digits=4))\n",
    "print('Confusion Matrix (PCA):\\n', confusion_matrix(y_test, pred_pca))\n",
    "\n",
    "pca_model = {'type': 'pca_recon', 'bundle': pca_bundle, 'thr': thr_pca}\n",
    "joblib.dump(pca_model, 'fraud_pca_anomaly.joblib')\n",
    "print('Saved ‚Üí fraud_pca_anomaly.joblib')\n",
    "\n",
    "# ---------------- Inference helpers (feature-order safe) ----------------\n",
    "def score_transactions_lr(df_tx: pd.DataFrame, threshold: Optional[float] = None) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    saved = joblib.load('fraud_lr_balanced.joblib')\n",
    "    model, thr, feats = saved['model'], saved['thr'], saved['feature_names']\n",
    "    df_tx = df_tx.reindex(columns=feats, fill_value=0)  # guard against drift\n",
    "    proba = model.predict_proba(df_tx.values)[:, 1]\n",
    "    t = threshold if threshold is not None else thr\n",
    "    return proba, (proba >= t).astype(int)\n",
    "\n",
    "def score_transactions_pca(df_tx: pd.DataFrame, threshold: Optional[float] = None) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    saved = joblib.load('fraud_pca_anomaly.joblib')\n",
    "    bndl, thr = saved['bundle'], saved['thr']\n",
    "    scores = pca_recon_error(bndl, df_tx)\n",
    "    scores = (scores - scores.min()) / (scores.max() - scores.min() + 1e-12)\n",
    "    t = threshold if threshold is not None else thr\n",
    "    return scores, (scores >= t).astype(int)\n",
    "\n",
    "def fuse_decisions(\n",
    "    proba_lr: np.ndarray,\n",
    "    score_pca: np.ndarray,\n",
    "    thr_lr: float,\n",
    "    thr_pca: float,\n",
    "    mode: Literal['or', 'and', 'avg'] = 'or',\n",
    "    w_lr: float = 0.6\n",
    ") -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Fusion:\n",
    "      - 'or' : flag if (LR>=thr_lr) OR (PCA>=thr_pca)        -> higher recall\n",
    "      - 'and': flag if (LR>=thr_lr) AND (PCA>=thr_pca)       -> higher precision\n",
    "      - 'avg': flag if w*LR + (1-w)*PCA >= fused_threshold   -> set fused_threshold=0.5 by default\n",
    "    Returns (fused_score, fused_pred)\n",
    "    \"\"\"\n",
    "    if mode == 'or':\n",
    "        pred = ((proba_lr >= thr_lr) | (score_pca >= thr_pca)).astype(int)\n",
    "        score = np.maximum(proba_lr, score_pca)\n",
    "        return score, pred\n",
    "    elif mode == 'and':\n",
    "        pred = ((proba_lr >= thr_lr) & (score_pca >= thr_pca)).astype(int)\n",
    "        score = np.minimum(proba_lr, score_pca)\n",
    "        return score, pred\n",
    "    else:  # 'avg'\n",
    "        fused = w_lr * proba_lr + (1 - w_lr) * score_pca\n",
    "        # pick a decent default fused threshold\n",
    "        fused_thr = 0.5\n",
    "        pred = (fused >= fused_thr).astype(int)\n",
    "        return fused, pred\n",
    "\n",
    "# ---------------- Example: how to use the saved artifacts ----------------\n",
    "if __name__ == \"__main__\":\n",
    "    # Single or batch scoring on the test fold:\n",
    "    proba_lr_test, pred_lr_test = score_transactions_lr(X_test)\n",
    "    score_pca_test, pred_pca_test = score_transactions_pca(X_test)\n",
    "\n",
    "    # Fusion examples\n",
    "    lr_thr = joblib.load('fraud_lr_balanced.joblib')['thr']\n",
    "    pca_thr = joblib.load('fraud_pca_anomaly.joblib')['thr']\n",
    "\n",
    "    fused_score_or, fused_pred_or   = fuse_decisions(proba_lr_test, score_pca_test, lr_thr, pca_thr, mode='or')\n",
    "    fused_score_and, fused_pred_and = fuse_decisions(proba_lr_test, score_pca_test, lr_thr, pca_thr, mode='and')\n",
    "    fused_score_avg, fused_pred_avg = fuse_decisions(proba_lr_test, score_pca_test, lr_thr, pca_thr, mode='avg', w_lr=0.7)\n",
    "\n",
    "    # Quick sanity printouts (optional)\n",
    "    from sklearn.metrics import classification_report\n",
    "    print(\"\\n--- Fusion=OR report (higher recall) ---\")\n",
    "    print(classification_report(y_test, fused_pred_or, digits=4))\n",
    "    print(\"\\n--- Fusion=AND report (higher precision) ---\")\n",
    "    print(classification_report(y_test, fused_pred_and, digits=4))\n",
    "    print(\"\\n--- Fusion=AVG report (w=0.7) ---\")\n",
    "    print(classification_report(y_test, fused_pred_avg, digits=4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a5bf336",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Credit Card Fraud Detector (Streamlit)\n",
    "# - Loads: fraud_lr_balanced.joblib, fraud_pca_anomaly.joblib\n",
    "# - Single transaction JSON or CSV upload\n",
    "# - Tunable thresholds + decision fusion (OR / AND / weighted average)\n",
    "# - Safe feature-ordering (prevents 'feature names' warnings)\n",
    "# - Downloadable results\n",
    "\n",
    "import json, os, joblib, numpy as np, pandas as pd, streamlit as st\n",
    "from typing import Tuple\n",
    "\n",
    "st.set_page_config(page_title=\"Fraud Detector\", page_icon=\"üí≥\", layout=\"wide\")\n",
    "st.title(\"üí≥ Credit Card Fraud Detector\")\n",
    "st.caption(\"Logistic Regression (balanced) + PCA anomaly ‚Ä¢ Kaggle CreditCardFraud dataset\")\n",
    "\n",
    "# -----------------------------\n",
    "# Load artifacts (cached)\n",
    "# -----------------------------\n",
    "@st.cache_resource\n",
    "def load_artifacts():\n",
    "    if not os.path.exists(\"fraud_lr_balanced.joblib\"):\n",
    "        raise FileNotFoundError(\"Missing fraud_lr_balanced.joblib. Train and save first.\")\n",
    "    if not os.path.exists(\"fraud_pca_anomaly.joblib\"):\n",
    "        raise FileNotFoundError(\"Missing fraud_pca_anomaly.joblib. Train and save first.\")\n",
    "    lr_bundle  = joblib.load(\"fraud_lr_balanced.joblib\")\n",
    "    pca_bundle = joblib.load(\"fraud_pca_anomaly.joblib\")\n",
    "    # Extract pieces\n",
    "    lr_model      = lr_bundle[\"model\"]\n",
    "    lr_thr_saved  = float(lr_bundle[\"thr\"])\n",
    "    feature_names = lr_bundle.get(\"feature_names\", None)\n",
    "    if feature_names is None:\n",
    "        # Backward safe: derive from model if missing\n",
    "        try:\n",
    "            feature_names = lr_model.named_steps[\"scaler\"].feature_names_in_.tolist()\n",
    "        except Exception:\n",
    "            raise ValueError(\"feature_names not found. Re-save with training column order.\")\n",
    "    pca_bndl = pca_bundle[\"bundle\"]\n",
    "    pca_thr  = float(pca_bundle[\"thr\"])\n",
    "    pca_feats = pca_bndl[\"feature_names\"]\n",
    "    # Must match columns; if not, we‚Äôll reindex to lr feature list\n",
    "    if feature_names != pca_feats:\n",
    "        st.warning(\"LR and PCA feature lists differ. The app will align inputs to LR‚Äôs feature_names.\")\n",
    "    return lr_model, lr_thr_saved, feature_names, pca_bndl, pca_thr\n",
    "\n",
    "LR_MODEL, LR_THR_SAVED, FEATS, PCA_BUNDLE, PCA_THR_SAVED = load_artifacts()\n",
    "\n",
    "# -----------------------------\n",
    "# Helpers (feature-order-safe)\n",
    "# -----------------------------\n",
    "def ensure_feature_order(df_like: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Return a DataFrame with exactly the training columns (missing -> 0, extras dropped).\"\"\"\n",
    "    if not isinstance(df_like, pd.DataFrame):\n",
    "        df_like = pd.DataFrame([df_like], columns=FEATS)  # best effort\n",
    "    # cast to numeric and align\n",
    "    df_like = df_like.copy()\n",
    "    for c in df_like.columns:\n",
    "        df_like[c] = pd.to_numeric(df_like[c], errors=\"coerce\")\n",
    "    df_like = df_like.reindex(columns=FEATS, fill_value=0)\n",
    "    return df_like\n",
    "\n",
    "def pca_recon_error(bundle: dict, X_df: pd.DataFrame) -> np.ndarray:\n",
    "    \"\"\"Compute PCA reconstruction error from saved components/scaler.\"\"\"\n",
    "    # enforce feature order for PCA as well (it stores its own list)\n",
    "    X_df = X_df.reindex(columns=bundle['feature_names'], fill_value=0)\n",
    "    Xv = X_df.values.astype(float)\n",
    "    center, scale = bundle['center_'], bundle['scale_']\n",
    "    Vt_k = bundle['Vt_k']\n",
    "    X_rb = (Xv - center) / (scale + 1e-12)\n",
    "    Z = X_rb @ Vt_k.T\n",
    "    X_rec = Z @ Vt_k\n",
    "    err = np.sum((X_rb - X_rec)**2, axis=1)\n",
    "    return err\n",
    "\n",
    "def minmax01(x: np.ndarray) -> np.ndarray:\n",
    "    return (x - x.min()) / (x.max() - x.min() + 1e-12)\n",
    "\n",
    "def score_lr(df_tx: pd.DataFrame, thr: float) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    X = ensure_feature_order(df_tx)\n",
    "    # Keep DataFrame to avoid feature-name warnings:\n",
    "    proba = LR_MODEL.predict_proba(X)[:, 1]\n",
    "    pred  = (proba >= thr).astype(int)\n",
    "    return proba, pred\n",
    "\n",
    "def score_pca(df_tx: pd.DataFrame, thr: float) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
    "    X = ensure_feature_order(df_tx)  # align to LR features; PCA helper reindexes again internally\n",
    "    err = pca_recon_error(PCA_BUNDLE, X)\n",
    "    score = minmax01(err)  # fraudiness in [0,1]\n",
    "    pred  = (score >= thr).astype(int)\n",
    "    return score, pred, err\n",
    "\n",
    "def fuse(proba_lr, score_pca, thr_lr, thr_pca, mode=\"or\", w_lr=0.6, fused_thr=0.5):\n",
    "    \"\"\"\n",
    "    Fusion:\n",
    "      - 'or'  : flag if LR>=thr_lr OR PCA>=thr_pca   (higher recall)\n",
    "      - 'and' : flag if LR>=thr_lr AND PCA>=thr_pca  (higher precision)\n",
    "      - 'avg' : flag if w_lr*LR + (1-w_lr)*PCA >= fused_thr\n",
    "    Returns (fused_score, fused_pred)\n",
    "    \"\"\"\n",
    "    mode = (mode or \"or\").lower()\n",
    "    if mode == \"and\":\n",
    "        pred = ((proba_lr >= thr_lr) & (score_pca >= thr_pca)).astype(int)\n",
    "        score = np.minimum(proba_lr, score_pca)\n",
    "        return score, pred\n",
    "    elif mode == \"avg\":\n",
    "        fused = w_lr * proba_lr + (1 - w_lr) * score_pca\n",
    "        pred  = (fused >= fused_thr).astype(int)\n",
    "        return fused, pred\n",
    "    else:  # 'or'\n",
    "        pred = ((proba_lr >= thr_lr) | (score_pca >= thr_pca)).astype(int)\n",
    "        score = np.maximum(proba_lr, score_pca)\n",
    "        return score, pred\n",
    "\n",
    "# -----------------------------\n",
    "# Sidebar controls\n",
    "# -----------------------------\n",
    "st.sidebar.header(\"‚öôÔ∏è Settings\")\n",
    "lr_thr   = st.sidebar.slider(\"LR threshold (saved default)\", 0.01, 0.99, float(LR_THR_SAVED), 0.01)\n",
    "pca_thr  = st.sidebar.slider(\"PCA threshold (saved default)\", 0.01, 0.99, float(PCA_THR_SAVED), 0.01)\n",
    "fusion   = st.sidebar.selectbox(\"Fusion mode\", [\"or\", \"and\", \"avg\"], index=0)\n",
    "w_lr     = st.sidebar.slider(\"Weighted fusion: weight for LR (w)\", 0.0, 1.0, 0.6, 0.05)\n",
    "f_thr    = st.sidebar.slider(\"Weighted fusion: fused threshold\", 0.05, 0.95, 0.50, 0.01)\n",
    "st.sidebar.markdown(\"**Artifacts:** `fraud_lr_balanced.joblib`, `fraud_pca_anomaly.joblib`\")\n",
    "st.sidebar.markdown(\"**Dataset:** Kaggle ‚Äì Credit Card Fraud (<small>ULB</small>)\")\n",
    "\n",
    "# -----------------------------\n",
    "# Tabs\n",
    "# -----------------------------\n",
    "tab1, tab2 = st.tabs([\"Single Transaction\", \"Batch CSV\"])\n",
    "\n",
    "# ===== Tab 1: Single =====\n",
    "with tab1:\n",
    "    st.subheader(\"Single Transaction (JSON)\")\n",
    "    colL, colR = st.columns([2,1])\n",
    "\n",
    "    with colL:\n",
    "        example = st.toggle(\"Load a random realistic example\")\n",
    "        if example:\n",
    "            # Simple synthetic demo in the V1..V28 + Amount + Time shape\n",
    "            ex = {c: 0 for c in FEATS}\n",
    "            # nudge a few features\n",
    "            for k in [\"V3\",\"V10\",\"V12\",\"V14\",\"V17\",\"V18\",\"V21\",\"Amount\"]:\n",
    "                if k in ex: ex[k] = float(np.random.normal(loc=2.0, scale=1.0))\n",
    "            st.session_state[\"single_json\"] = json.dumps(ex, indent=2)\n",
    "        default_json = st.session_state.get(\"single_json\", \"{\\n  \\\"\" + FEATS[0] + \"\\\": 0\\n}\")\n",
    "        txt = st.text_area(\n",
    "            \"Paste a JSON object with the same schema as the training features.\",\n",
    "            value=default_json, height=280\n",
    "        )\n",
    "        run_single = st.button(\"Score Single\", type=\"primary\")\n",
    "    with colR:\n",
    "        st.write(\"**Training feature names**\")\n",
    "        st.code(\", \".join(FEATS[:10]) + (\" ... \" if len(FEATS) > 10 else \"\"), language=\"text\")\n",
    "\n",
    "    if run_single:\n",
    "        try:\n",
    "            data = json.loads(txt)\n",
    "            if isinstance(data, dict):\n",
    "                df_one = pd.DataFrame([data])\n",
    "            elif isinstance(data, list):\n",
    "                df_one = pd.DataFrame(data)\n",
    "            else:\n",
    "                raise ValueError(\"JSON must be an object or list of objects.\")\n",
    "        except Exception as e:\n",
    "            st.error(f\"Invalid JSON: {e}\")\n",
    "            st.stop()\n",
    "\n",
    "        proba_lr, pred_lr = score_lr(df_one, thr=lr_thr)\n",
    "        score_pc, pred_pc, err = score_pca(df_one, thr=pca_thr)\n",
    "        fused_score, fused_pred = fuse(proba_lr, score_pc, lr_thr, pca_thr, mode=fusion, w_lr=w_lr, fused_thr=f_thr)\n",
    "\n",
    "        c1, c2, c3 = st.columns(3)\n",
    "        with c1:\n",
    "            st.metric(\"LR prob (fraud)\", f\"{proba_lr[0]:.4f}\", delta=f\"thr {lr_thr:.2f}\")\n",
    "            st.write(\"Decision:\", \"**FRAUD**\" if pred_lr[0]==1 else \"legit\")\n",
    "        with c2:\n",
    "            st.metric(\"PCA score (fraudiness)\", f\"{score_pc[0]:.4f}\", delta=f\"thr {pca_thr:.2f}\")\n",
    "            st.write(\"Decision:\", \"**FRAUD**\" if pred_pc[0]==1 else \"legit\")\n",
    "        with c3:\n",
    "            st.metric(\"Fused score\", f\"{fused_score[0]:.4f}\", delta=f\"{fusion} mode\")\n",
    "            st.write(\"Decision:\", \"**FRAUD**\" if fused_pred[0]==1 else \"legit\")\n",
    "\n",
    "        st.markdown(\"**Verbose JSON**\")\n",
    "        st.json({\n",
    "            \"lr\": {\"proba_fraud\": float(proba_lr[0]), \"threshold\": lr_thr, \"pred\": int(pred_lr[0])},\n",
    "            \"pca\": {\"score_fraud\": float(score_pc[0]), \"threshold\": pca_thr, \"pred\": int(pred_pc[0])},\n",
    "            \"fusion\": {\"mode\": fusion, \"w_lr\": w_lr, \"fused_thr\": f_thr,\n",
    "                       \"score\": float(fused_score[0]), \"pred\": int(fused_pred[0])}\n",
    "        })\n",
    "\n",
    "# ===== Tab 2: Batch =====\n",
    "with tab2:\n",
    "    st.subheader(\"Batch CSV scoring\")\n",
    "    st.write(\"Upload a CSV with **exactly** the model‚Äôs feature columns (extras are ignored, missing are filled with 0).\")\n",
    "    up = st.file_uploader(\"CSV file\", type=[\"csv\"])\n",
    "    if up is not None:\n",
    "        df_in = pd.read_csv(up)\n",
    "        st.write(\"Preview:\")\n",
    "        st.dataframe(df_in.head(), use_container_width=True)\n",
    "\n",
    "        if st.button(\"Score CSV\", type=\"primary\"):\n",
    "            X = ensure_feature_order(df_in)\n",
    "            proba_lr, pred_lr = score_lr(X, thr=lr_thr)\n",
    "            score_pc, pred_pc, err = score_pca(X, thr=pca_thr)\n",
    "            fused_score, fused_pred = fuse(proba_lr, score_pc, lr_thr, pca_thr, mode=fusion, w_lr=w_lr, fused_thr=f_thr)\n",
    "\n",
    "            out = df_in.copy()\n",
    "            out[\"lr_proba_fraud\"] = proba_lr\n",
    "            out[\"lr_pred\"] = pred_lr\n",
    "            out[\"pca_score_fraud\"] = score_pc\n",
    "            out[\"pca_pred\"] = pred_pc\n",
    "            out[\"fused_score\"] = fused_score\n",
    "            out[\"fused_pred\"] = fused_pred\n",
    "\n",
    "            st.success(\"Scoring complete.\")\n",
    "            st.dataframe(out.head(30), use_container_width=True)\n",
    "            st.download_button(\n",
    "                \"Download results (CSV)\",\n",
    "                data=out.to_csv(index=False).encode(\"utf-8\"),\n",
    "                file_name=\"fraud_scored.csv\",\n",
    "                mime=\"text/csv\",\n",
    "                use_container_width=True\n",
    "            )\n",
    "\n",
    "# -----------------------------\n",
    "# Explainers / Notes\n",
    "# -----------------------------\n",
    "st.markdown(\"---\")\n",
    "st.markdown(\"\"\"\n",
    "**How this works**\n",
    "\n",
    "- **LR (supervised):** outputs a calibrated-like probability of fraud. We use the **PR-optimal threshold** learned during training (you can adjust it).\n",
    "- **PCA (anomaly):** trains on normal transactions only; **reconstruction error** is min-max scaled to a fraudiness score in [0,1].\n",
    "- **Fusion:**\n",
    "  - **OR** ‚Üí alert if either LR or PCA fires (higher recall).\n",
    "  - **AND** ‚Üí alert only if both fire (higher precision).\n",
    "  - **AVG** ‚Üí use weighted average of LR and PCA scores and compare to a fused threshold.\n",
    "\n",
    "**Best practices**\n",
    "- Keep thresholds stable across environments for consistent alerting.\n",
    "- Monitor drift: if incoming feature distributions shift, retrain or re-fit PCA on recent normals.\n",
    "- Log decisions with scores and thresholds for auditability.\n",
    "\n",
    "**Dataset**\n",
    "- Kaggle (ULB): https://www.kaggle.com/datasets/mlg-ulb/creditcardfraud\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0cdffc3",
   "metadata": {},
   "source": [
    "## Personalized Movie Recommendation (User-based CF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97e3e427",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö° Converting ratings.csv ‚Üí Parquet (1st-time)‚Ä¶\n",
      "‚ö° Converting movies.csv ‚Üí Parquet (1st-time)‚Ä¶\n",
      "‚ö° Building CSR rating matrix‚Ä¶\n",
      "‚ö° L2-normalizing users & items‚Ä¶\n",
      "Leave-One-Out Recall@10: 0.000\n",
      "‚úÖ  Saved ‚Üí movie_recommender.joblib\n",
      "\n",
      "Top-10 (hybrid) for user 1\n",
      "   movieId                                              title\n",
      "0      260          Star Wars: Episode IV - A New Hope (1977)\n",
      "1       50                         Usual Suspects, The (1995)\n",
      "2     2571                                 Matrix, The (1999)\n",
      "3     1196  Star Wars: Episode V - The Empire Strikes Back...\n",
      "4      318                   Shawshank Redemption, The (1994)\n",
      "5      593                   Silence of the Lambs, The (1991)\n",
      "6     1210  Star Wars: Episode VI - Return of the Jedi (1983)\n",
      "7      527                            Schindler's List (1993)\n",
      "8      110                                  Braveheart (1995)\n",
      "9      356                                Forrest Gump (1994)\n",
      "\n",
      "Movies similar to: Toy Story (1995)\n",
      "   movieId                           title\n",
      "0   209069                Snapshots (2002)\n",
      "1   209085     The Mistletoe Secret (2019)\n",
      "2   209079         Call of Cuteness (2017)\n",
      "3   209075  Monkey Love Experiments (2014)\n",
      "4   209163                Bad Poems (2018)\n",
      "5   209169             A Girl Thing (2001)\n",
      "6        1                Toy Story (1995)\n",
      "7   209171  Women of Devil's Island (1962)\n",
      "8   209073             The Arbalest (2016)\n",
      "9   209057                The Somme (2005)\n"
     ]
    }
   ],
   "source": [
    "# ===========================================================\n",
    "#  Movie Recommender ‚Äì High-throughput Edition\n",
    "#  * One-time build ‚âà 60-90 s on 16 GB / 8-Core laptop\n",
    "#  * Subsequent runs ‚âà 2-3 s (load Joblib + serve requests)\n",
    "# ===========================================================\n",
    "\n",
    "import os, sys, math, json, shutil, joblib, numpy as np, pandas as pd\n",
    "from pathlib import Path\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from scipy.sparse import csr_matrix, save_npz, load_npz\n",
    "from sklearn.preprocessing import normalize\n",
    "from collections import defaultdict\n",
    "\n",
    "# -------- Runtime tunables (override via env) ------------------\n",
    "N_THREADS = int(os.getenv(\"NUM_THREADS\", max(os.cpu_count()-1, 1)))\n",
    "os.environ[\"OPENBLAS_NUM_THREADS\"] = os.environ[\"MKL_NUM_THREADS\"] = str(N_THREADS)\n",
    "\n",
    "DATA_DIR   = Path(\"data/Movie\")\n",
    "CACHE_DIR  = Path(\"movie_cache_fast\");  CACHE_DIR.mkdir(exist_ok=True)\n",
    "MODEL_PATH = Path(\"movie_recommender.joblib\")\n",
    "\n",
    "RATINGS_CSV = DATA_DIR / \"ratings.csv\"\n",
    "MOVIES_CSV  = DATA_DIR / \"movies.csv\"\n",
    "R_PARQUET   = CACHE_DIR / \"ratings.parquet\"\n",
    "M_PARQUET   = CACHE_DIR / \"movies.parquet\"\n",
    "CSR_NPZ     = CACHE_DIR / \"ratings_csr.npz\"\n",
    "MAPS_PKL    = CACHE_DIR / \"id_maps.pkl\"\n",
    "\n",
    "# --------------------------------------------------------------\n",
    "# 1) Fast CSV ‚Üí Parquet (runs once)\n",
    "# --------------------------------------------------------------\n",
    "def _csv_to_parquet(src: Path, dst: Path, dtypes: dict):\n",
    "    try:\n",
    "        import polars as pl\n",
    "        df = pl.read_csv(src, dtypes=dtypes).to_pandas()\n",
    "    except ModuleNotFoundError:\n",
    "        df = pd.read_csv(src, dtype=dtypes)\n",
    "    dst.parent.mkdir(exist_ok=True)\n",
    "    df.to_parquet(dst, index=False)\n",
    "    return df\n",
    "\n",
    "if not R_PARQUET.exists():\n",
    "    print(\"‚ö° Converting ratings.csv ‚Üí Parquet (1st-time)‚Ä¶\")\n",
    "    _csv_to_parquet(\n",
    "        RATINGS_CSV, R_PARQUET,\n",
    "        dtypes={\"userId\": \"int32\", \"movieId\": \"int32\", \"rating\": \"float32\", \"timestamp\": \"int64\"}\n",
    "    )\n",
    "if not M_PARQUET.exists():\n",
    "    print(\"‚ö° Converting movies.csv ‚Üí Parquet (1st-time)‚Ä¶\")\n",
    "    _csv_to_parquet(\n",
    "        MOVIES_CSV, M_PARQUET,\n",
    "        dtypes={\"movieId\": \"int32\", \"title\": \"string\", \"genres\": \"string\"}\n",
    "    )\n",
    "\n",
    "# --------------------------------------------------------------\n",
    "# 2) Load Parquet (<< 1 s)\n",
    "# --------------------------------------------------------------\n",
    "df_ratings = pd.read_parquet(R_PARQUET)\n",
    "df_movies  = pd.read_parquet(M_PARQUET)\n",
    "\n",
    "# --------------------------------------------------------------\n",
    "# 3) Build csr_matrix once, cache to NPZ\n",
    "# --------------------------------------------------------------\n",
    "if not CSR_NPZ.exists() or not MAPS_PKL.exists():\n",
    "    print(\"‚ö° Building CSR rating matrix‚Ä¶\")\n",
    "    uid_map = {u: i for i, u in enumerate(np.sort(df_ratings[\"userId\"].unique()))}\n",
    "    iid_map = {m: j for j, m in enumerate(np.sort(df_ratings[\"movieId\"].unique()))}\n",
    "    rows = df_ratings[\"userId\"].map(uid_map).to_numpy(\"int32\")\n",
    "    cols = df_ratings[\"movieId\"].map(iid_map).to_numpy(\"int32\")\n",
    "    vals = df_ratings[\"rating\"].to_numpy(\"float32\")\n",
    "    R = csr_matrix((vals, (rows, cols)),\n",
    "                   shape=(len(uid_map), len(iid_map)), dtype=np.float32)\n",
    "    save_npz(CSR_NPZ, R)\n",
    "    joblib.dump({\"uid_map\": uid_map, \"iid_map\": iid_map}, MAPS_PKL, compress=3)\n",
    "else:\n",
    "    uid_map = joblib.load(MAPS_PKL)[\"uid_map\"]\n",
    "    iid_map = joblib.load(MAPS_PKL)[\"iid_map\"]\n",
    "    R = load_npz(CSR_NPZ)\n",
    "\n",
    "uid_inv = {i: u for u, i in uid_map.items()}\n",
    "iid_inv = {j: m for m, j in iid_map.items()}\n",
    "\n",
    "# --------------------------------------------------------------\n",
    "# 4) Cache L2-normalized copies\n",
    "# --------------------------------------------------------------\n",
    "print(\"‚ö° L2-normalizing users & items‚Ä¶\")\n",
    "R_user = normalize(R, axis=1, copy=True)\n",
    "R_item = normalize(R, axis=0, copy=True)\n",
    "\n",
    "# --------------------------------------------------------------\n",
    "# 5) Popularity prior (Bayesian smoothing, m = 20)\n",
    "# --------------------------------------------------------------\n",
    "stats = df_ratings.groupby(\"movieId\").rating.agg([\"count\", \"mean\"]).reset_index()\n",
    "gmean = df_ratings[\"rating\"].mean()\n",
    "m = 20\n",
    "stats[\"pop_score\"] = (stats[\"count\"]*stats[\"mean\"] + m*gmean) / (stats[\"count\"] + m)\n",
    "pop_items_internal = [iid_map[i] for i in stats.sort_values(\"pop_score\", ascending=False)[\"movieId\"]\n",
    "                      if i in iid_map]\n",
    "\n",
    "# --------------------------------------------------------------\n",
    "# 6) Core scorers  (vectorized dot-product ‚Üí BLAS multi-thread)\n",
    "# --------------------------------------------------------------\n",
    "def _usercf(uidx: int, k=50):\n",
    "    sims = R_user @ R_user[uidx].T      # (N_users, 1)\n",
    "    sims = sims.toarray().ravel()\n",
    "    sims[uidx] = 0\n",
    "    neigh = sims.argpartition(-k)[:k]\n",
    "    scores = defaultdict(float)\n",
    "    seen = set(R[uidx].indices)\n",
    "    for n in neigh:\n",
    "        s = sims[n]\n",
    "        if s <= 0: continue\n",
    "        row = R[n]\n",
    "        for j, v in zip(row.indices, row.data):\n",
    "            if j in seen: continue\n",
    "            scores[j] += s * v\n",
    "    return scores\n",
    "\n",
    "def _itemcf(uidx: int, k=100):\n",
    "    seen_idx = R[uidx].indices\n",
    "    seen_val = R[uidx].data\n",
    "    if len(seen_idx) == 0:\n",
    "        return {}\n",
    "    scores = defaultdict(float)\n",
    "    for it, v in zip(seen_idx, seen_val):\n",
    "        sims = R_item.T @ R_item[:, it]\n",
    "        sims = sims.toarray().ravel()\n",
    "        sims[it] = 0\n",
    "        neigh = sims.argpartition(-k)[:k]\n",
    "        for j in neigh:\n",
    "            if j in seen_idx: continue\n",
    "            scores[j] += sims[j] * v\n",
    "    return scores\n",
    "\n",
    "def _hybrid(uidx, alpha=0.6, ku=50, ki=100):\n",
    "    sc_u = _usercf(uidx, ku)\n",
    "    sc_i = _itemcf(uidx, ki)\n",
    "    items = set(sc_u) | set(sc_i)\n",
    "    return {i: alpha*sc_u.get(i, 0) + (1-alpha)*sc_i.get(i, 0) for i in items}\n",
    "\n",
    "# --------------------------------------------------------------\n",
    "# 7) Public API\n",
    "# --------------------------------------------------------------\n",
    "def recommend_for_user(user_id: int, top_n=10, mode=\"hybrid\",\n",
    "                       alpha=0.6, ku=50, ki=100):\n",
    "    if user_id not in uid_map:\n",
    "        rec_idx = pop_items_internal[:top_n]\n",
    "    else:\n",
    "        uidx = uid_map[user_id]\n",
    "        if mode == \"user\":\n",
    "            scores = _usercf(uidx, ku)\n",
    "        elif mode == \"item\":\n",
    "            scores = _itemcf(uidx, ki)\n",
    "        else:\n",
    "            scores = _hybrid(uidx, alpha, ku, ki)\n",
    "        rec_idx = sorted(scores, key=scores.get, reverse=True)[:top_n] or pop_items_internal[:top_n]\n",
    "    mids = [iid_inv[i] for i in rec_idx]\n",
    "    return df_movies[df_movies.movieId.isin(mids)][[\"movieId\", \"title\"]].set_index(\"movieId\").loc[mids].reset_index()\n",
    "\n",
    "def similar_movies(movie_id: int, top_n=10):\n",
    "    if movie_id not in iid_map:\n",
    "        return pd.DataFrame()\n",
    "    itx = iid_map[movie_id]\n",
    "    sims = R_item.T @ R_item[:, itx]\n",
    "    sims = sims.toarray().ravel()\n",
    "    sims[itx] = 0\n",
    "    neigh = sims.argpartition(-top_n)[:top_n]\n",
    "    neigh = neigh[np.argsort(-sims[neigh])]\n",
    "    mids = [iid_inv[i] for i in neigh]\n",
    "    return df_movies[df_movies.movieId.isin(mids)][[\"movieId\", \"title\"]].set_index(\"movieId\").loc[mids].reset_index()\n",
    "\n",
    "# --------------------------------------------------------------\n",
    "# 8) Minimal evaluation (LOO Recall@10 on 200 users)\n",
    "#    Runs quickly thanks to pre-built matrices.\n",
    "# --------------------------------------------------------------\n",
    "def loo_recall(k=10, max_users=200):\n",
    "    rng = np.random.default_rng(42)\n",
    "    hits = 0; tested = 0\n",
    "    for uid, grp in df_ratings.groupby(\"userId\"):\n",
    "        if len(grp) < 3: continue\n",
    "        tested += 1\n",
    "        if tested > max_users: break\n",
    "        hold = grp.sample(1, random_state=rng).iloc[0]\n",
    "        recommendations = recommend_for_user(uid, top_n=k).movieId.tolist()\n",
    "        if hold.movieId in recommendations:\n",
    "            hits += 1\n",
    "    return hits / tested if tested else None\n",
    "\n",
    "recall10 = loo_recall()\n",
    "print(f\"Leave-One-Out Recall@10: {recall10:.3f}\")\n",
    "\n",
    "# --------------------------------------------------------------\n",
    "# 9) Persist complete model bundle\n",
    "# --------------------------------------------------------------\n",
    "model_bundle = dict(\n",
    "    R_path=str(CSR_NPZ), R_user_norm=None, R_item_norm=None,  # load lazily if huge\n",
    "    uid_map=uid_map, iid_map=iid_map, uid_inv=uid_inv, iid_inv=iid_inv,\n",
    "    df_movies=df_movies[[\"movieId\", \"title\", \"genres\"]],\n",
    "    pop_idx=pop_items_internal,\n",
    "    meta=dict(recall10=recall10, threads=N_THREADS)\n",
    ")\n",
    "joblib.dump(model_bundle, MODEL_PATH, compress=3)\n",
    "print(\"‚úÖ  Saved ‚Üí\", MODEL_PATH)\n",
    "\n",
    "# ------------------- Quick demo -------------------\n",
    "if __name__ == \"__main__\":\n",
    "    uid_demo = list(uid_map.keys())[0]\n",
    "    print(\"\\nTop-10 (hybrid) for user\", uid_demo)\n",
    "    print(recommend_for_user(uid_demo, top_n=10))\n",
    "\n",
    "    mid_demo = list(iid_map.keys())[0]\n",
    "    print(\"\\nMovies similar to:\", df_movies.loc[df_movies.movieId==mid_demo, 'title'].iloc[0])\n",
    "    print(similar_movies(mid_demo, top_n=10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a23b5de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# app_movie_streamlit.py\n",
    "# ===========================================================\n",
    "#  üé¨ Streamlit Movie Recommender (Auto-Personal + Filters + APIs)\n",
    "#  - Loads bundle: movie_recommender.joblib (created by your trainer script)\n",
    "#  - Personalizes WITHOUT asking questions:\n",
    "#      ‚Ä¢ If URL has ?user_id=<id> and it's in data ‚Üí use that profile\n",
    "#      ‚Ä¢ Otherwise start from popularity; learns from your üëç during session\n",
    "#  - Advanced controls: include/exclude genres, year range, min rating count\n",
    "#  - Recency boost, Serendipity (exploration), MMR diversity re-ranking\n",
    "#  - Optional TMDb (posters) & OMDb (plot/IMDB rating) ‚Äî paste keys in sidebar\n",
    "#  - Uses st.query_params (no deprecated experimental API)\n",
    "# ===========================================================\n",
    "\n",
    "import os, re, time, json, math, numpy as np, pandas as pd, joblib, requests\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple\n",
    "from scipy.sparse import load_npz, csr_matrix\n",
    "from sklearn.preprocessing import normalize\n",
    "import streamlit as st\n",
    "\n",
    "# -------------------- Page setup --------------------\n",
    "st.set_page_config(page_title=\"üé¨ Movie Recommender\", page_icon=\"üé¨\", layout=\"wide\")\n",
    "st.title(\"üé¨ Movie Recommender\")\n",
    "st.caption(\"Hybrid collaborative filtering ‚Ä¢ Auto-personalization ‚Ä¢ Filters ‚Ä¢ Diversity ‚Ä¢ Recency ‚Ä¢ APIs\")\n",
    "\n",
    "# -------------------- Load model bundle --------------------\n",
    "@st.cache_resource\n",
    "def load_bundle(path: str = \"movie_recommender.joblib\"):\n",
    "    if not Path(path).exists():\n",
    "        raise FileNotFoundError(\n",
    "            f\"Missing '{path}'. Run the training notebook/script that saved movie_recommender.joblib.\"\n",
    "        )\n",
    "    B = joblib.load(path)\n",
    "    for k in [\"R_path\",\"uid_map\",\"iid_map\",\"uid_inv\",\"iid_inv\",\"df_movies\",\"pop_idx\",\"meta\"]:\n",
    "        if k not in B:\n",
    "            raise ValueError(f\"Bundle missing key: {k}\")\n",
    "    return B\n",
    "\n",
    "B = load_bundle()\n",
    "DF_MOVIES: pd.DataFrame = B[\"df_movies\"].copy()\n",
    "UID_MAP: Dict[int,int]   = B[\"uid_map\"]\n",
    "IID_MAP: Dict[int,int]   = B[\"iid_map\"]\n",
    "UID_INV: Dict[int,int]   = B[\"uid_inv\"]\n",
    "IID_INV: Dict[int,int]   = B[\"iid_inv\"]\n",
    "POP_IDX: List[int]       = B[\"pop_idx\"]\n",
    "R_NPZ                    = B[\"R_path\"]\n",
    "\n",
    "# -------------------- Load matrices (cached) --------------------\n",
    "@st.cache_resource\n",
    "def load_mats(r_npz_path: str) -> Tuple[csr_matrix, csr_matrix, csr_matrix, np.ndarray]:\n",
    "    R: csr_matrix = load_npz(r_npz_path)\n",
    "    R_user = normalize(R, axis=1, copy=True)  # user-cosine\n",
    "    R_item = normalize(R, axis=0, copy=True)  # item-cosine\n",
    "    item_counts = np.asarray((R > 0).sum(axis=0)).ravel()  # #ratings per movie\n",
    "    return R, R_user, R_item, item_counts\n",
    "\n",
    "R, R_USER, R_ITEM, ITEM_COUNTS = load_mats(R_NPZ)\n",
    "\n",
    "# -------------------- Metadata (genres, years) --------------------\n",
    "def parse_year(title: str) -> int | None:\n",
    "    m = re.search(r\"\\((\\d{4})\\)\\s*$\", str(title))\n",
    "    return int(m.group(1)) if m else None\n",
    "\n",
    "DF_MOVIES[\"year\"] = DF_MOVIES[\"title\"].map(parse_year)\n",
    "\n",
    "def split_genres(g: str) -> List[str]:\n",
    "    g = (g or \"\")\n",
    "    return [] if g in (\"(no genres listed)\", \"\", None) else [x.strip() for x in g.split(\"|\")]\n",
    "\n",
    "DF_MOVIES[\"genres_list\"] = DF_MOVIES[\"genres\"].map(split_genres)\n",
    "ALL_GENRES = sorted({g for gs in DF_MOVIES[\"genres_list\"] for g in gs})\n",
    "\n",
    "TITLE_BY_MID = dict(zip(DF_MOVIES.movieId, DF_MOVIES.title))\n",
    "GENRES_BY_MID = dict(zip(DF_MOVIES.movieId, DF_MOVIES.genres_list))\n",
    "YEAR_BY_MID = dict(zip(DF_MOVIES.movieId, DF_MOVIES.year))\n",
    "\n",
    "# -------------------- Similarity-based scorers --------------------\n",
    "def _user_seen(uidx: int) -> set[int]:\n",
    "    return set(R.getrow(uidx).indices.tolist())\n",
    "\n",
    "def _usercf_scores(uidx: int, k_neighbors=50) -> Dict[int,float]:\n",
    "    sims = (R_USER @ R_USER.getrow(uidx).T).toarray().ravel()\n",
    "    sims[uidx] = 0.0\n",
    "    if k_neighbors < len(sims):\n",
    "        neigh_idx = np.argpartition(-sims, k_neighbors)[:k_neighbors]\n",
    "    else:\n",
    "        neigh_idx = np.where(sims > 0)[0]\n",
    "    neigh_sims = sims[neigh_idx]\n",
    "    seen = _user_seen(uidx)\n",
    "    scores: Dict[int,float] = {}\n",
    "    for n_i, s in zip(neigh_idx, neigh_sims):\n",
    "        if s <= 0: \n",
    "            continue\n",
    "        row = R.getrow(n_i)\n",
    "        for j, v in zip(row.indices, row.data):\n",
    "            if j in seen:\n",
    "                continue\n",
    "            scores[j] = scores.get(j, 0.0) + s * float(v)\n",
    "    return scores\n",
    "\n",
    "def _itemcf_scores(uidx: int, k_neighbors=100) -> Dict[int,float]:\n",
    "    row = R.getrow(uidx)\n",
    "    seen_idx, seen_val = row.indices, row.data\n",
    "    if len(seen_idx) == 0:\n",
    "        return {}\n",
    "    scores: Dict[int,float] = {}\n",
    "    for it, r in zip(seen_idx, seen_val):\n",
    "        sims = (R_ITEM.T @ R_ITEM[:, it]).toarray().ravel()\n",
    "        sims[it] = 0.0\n",
    "        if k_neighbors < len(sims):\n",
    "            neigh_idx = np.argpartition(-sims, k_neighbors)[:k_neighbors]\n",
    "        else:\n",
    "            neigh_idx = np.where(sims > 0)[0]\n",
    "        for jt in neigh_idx:\n",
    "            if jt in seen_idx:\n",
    "                continue\n",
    "            scores[jt] = scores.get(jt, 0.0) + float(sims[jt]) * float(r)\n",
    "    return scores\n",
    "\n",
    "def _hybrid_scores(uidx: int, ku=50, ki=100, alpha=0.6) -> Dict[int,float]:\n",
    "    su = _usercf_scores(uidx, ku)\n",
    "    si = _itemcf_scores(uidx, ki)\n",
    "    keys = set(su) | set(si)\n",
    "    return {k: alpha*su.get(k,0.0) + (1-alpha)*si.get(k,0.0) for k in keys}\n",
    "\n",
    "def _idx_to_frame(idx_list: List[int]) -> pd.DataFrame:\n",
    "    mids = [IID_INV[i] for i in idx_list]\n",
    "    out = DF_MOVIES[DF_MOVIES.movieId.isin(mids)][[\"movieId\",\"title\",\"genres\",\"year\"]].copy()\n",
    "    return out.set_index(\"movieId\").loc[mids].reset_index()\n",
    "\n",
    "# -------------------- Auto-profile (NO questions) --------------------\n",
    "# Source 1: URL param ?user_id=<id> (dataset user)\n",
    "# Source 2: Session üëç likes (implicit)\n",
    "params = st.query_params  # ‚úîÔ∏è modern API\n",
    "USER_ID_PARAM = None\n",
    "try:\n",
    "    if \"user_id\" in params and params.get(\"user_id\"):\n",
    "        # st.query_params values are strings (or list-like); handle both\n",
    "        raw = params.get(\"user_id\")\n",
    "        USER_ID_PARAM = int(raw[0] if isinstance(raw, list) else raw)\n",
    "except Exception:\n",
    "    USER_ID_PARAM = None\n",
    "\n",
    "@st.cache_data\n",
    "def user_genre_affinity_from_dataset(user_id: int) -> Dict[str, float]:\n",
    "    \"\"\"Normalized genre profile from this dataset user's historical ratings.\"\"\"\n",
    "    if user_id not in UID_MAP:\n",
    "        return {}\n",
    "    uidx = UID_MAP[user_id]\n",
    "    row = R.getrow(uidx)\n",
    "    w = {}\n",
    "    for j, rating in zip(row.indices, row.data):\n",
    "        mid = IID_INV[j]\n",
    "        for g in GENRES_BY_MID.get(mid, []):\n",
    "            w[g] = w.get(g, 0.0) + float(rating)\n",
    "    s = sum(w.values()) or 1.0\n",
    "    return {k: v/s for k, v in w.items()}\n",
    "\n",
    "if \"session_likes\" not in st.session_state:\n",
    "    st.session_state.session_likes: set[int] = set()\n",
    "\n",
    "def session_genre_affinity() -> Dict[str,float]:\n",
    "    \"\"\"Genre counts from quick likes this session.\"\"\"\n",
    "    if not st.session_state.session_likes:\n",
    "        return {}\n",
    "    w = {}\n",
    "    for mid in st.session_state.session_likes:\n",
    "        for g in GENRES_BY_MID.get(mid, []):\n",
    "            w[g] = w.get(g, 0.0) + 1.0\n",
    "    s = sum(w.values()) or 1.0\n",
    "    return {k: v/s for k, v in w.items()}\n",
    "\n",
    "def boost_by_genre(scores: Dict[int,float], beta: float, affinity: Dict[str,float]) -> Dict[int,float]:\n",
    "    \"\"\"Multiply score by (1 + Œ≤ * affinity_sum_of_item_genres).\"\"\"\n",
    "    if beta <= 0 or not affinity:\n",
    "        return scores\n",
    "    out = {}\n",
    "    for j, sc in scores.items():\n",
    "        mid = IID_INV[j]\n",
    "        a = sum(affinity.get(g, 0.0) for g in GENRES_BY_MID.get(mid, []))\n",
    "        out[j] = sc * (1.0 + beta * a)\n",
    "    return out\n",
    "\n",
    "# -------------------- Filters, Recency, Serendipity & MMR --------------------\n",
    "def apply_filters(candidates: List[int],\n",
    "                  include_genres: List[str],\n",
    "                  exclude_genres: List[str],\n",
    "                  year_min: int | None, year_max: int | None,\n",
    "                  min_count: int) -> List[int]:\n",
    "    keep: List[int] = []\n",
    "    for j in candidates:\n",
    "        mid = IID_INV[j]\n",
    "        gs  = GENRES_BY_MID.get(mid, [])\n",
    "        yr  = YEAR_BY_MID.get(mid, None)\n",
    "        cnt = int(ITEM_COUNTS[j])\n",
    "        if include_genres and not any(g in gs for g in include_genres):\n",
    "            continue\n",
    "        if exclude_genres and any(g in gs for g in exclude_genres):\n",
    "            continue\n",
    "        if year_min and yr and yr < year_min: \n",
    "            continue\n",
    "        if year_max and yr and yr > year_max:\n",
    "            continue\n",
    "        if cnt < min_count:\n",
    "            continue\n",
    "        keep.append(j)\n",
    "    return keep\n",
    "\n",
    "def recency_weight(year: int | None, ref_year: int, strength: float) -> float:\n",
    "    \"\"\"Return weight ‚àà (0, 1.5] that increases for newer titles; strength‚àà[0,1].\"\"\"\n",
    "    if not year or strength <= 0:\n",
    "        return 1.0\n",
    "    # Exponential decay back 30 years as a rough window\n",
    "    age = max(0, ref_year - year)\n",
    "    base = math.exp(-age / 30.0)  # ~0.97 per year\n",
    "    return 1.0 + strength * (base - 0.5)  # shift so mid‚âà1, cap via strength\n",
    "\n",
    "def apply_recency(scores: Dict[int,float], strength: float) -> Dict[int,float]:\n",
    "    if strength <= 0:\n",
    "        return scores\n",
    "    ref_year = int(pd.Timestamp.now().year)\n",
    "    out = {}\n",
    "    for j, sc in scores.items():\n",
    "        mid = IID_INV[j]\n",
    "        out[j] = sc * recency_weight(YEAR_BY_MID.get(mid), ref_year, strength)\n",
    "    return out\n",
    "\n",
    "def add_serendipity(scores: Dict[int,float], epsilon: float, seed: int = 42) -> Dict[int,float]:\n",
    "    \"\"\"Add tiny jitter to surface long-tail items; epsilon‚àà[0,1].\"\"\"\n",
    "    if epsilon <= 0:\n",
    "        return scores\n",
    "    rng = np.random.default_rng(seed + int(time.time()) // 30)  # refresh every ~30s\n",
    "    out = {}\n",
    "    for j, sc in scores.items():\n",
    "        noise = rng.random() * epsilon * 0.05  # up to 5% bump at epsilon=1\n",
    "        out[j] = sc * (1.0 + noise)\n",
    "    return out\n",
    "\n",
    "def mmr_diversify(ranked_idxs: List[int], top_n: int, lam: float = 0.7) -> List[int]:\n",
    "    \"\"\"\n",
    "    MMR: argmax_i [ Œª * rel(i) - (1-Œª) * max_{j‚ààS} sim(i,j) ]\n",
    "    rel = rank-based; sim via item cosine. Œª‚Üë = relevance, Œª‚Üì = diversity.\n",
    "    \"\"\"\n",
    "    if top_n <= 1 or not ranked_idxs:\n",
    "        return ranked_idxs[:top_n]\n",
    "    rel = {j: (len(ranked_idxs) - r) / len(ranked_idxs) for r, j in enumerate(ranked_idxs)}\n",
    "    S: List[int] = [ranked_idxs[0]]\n",
    "    C = set(ranked_idxs[1:])\n",
    "    while len(S) < min(top_n, len(ranked_idxs)) and C:\n",
    "        best, best_val = None, -1e9\n",
    "        for i in list(C):\n",
    "            # similarity to set S\n",
    "            sim_to_S = 0.0\n",
    "            for j in S:\n",
    "                sim_to_S = max(sim_to_S, float((R_ITEM[:, i].T @ R_ITEM[:, j]).toarray().ravel()[0]))\n",
    "            val = lam * rel.get(i, 0.0) - (1.0 - lam) * sim_to_S\n",
    "            if val > best_val:\n",
    "                best_val, best = val, i\n",
    "        S.append(best); C.remove(best)\n",
    "    return S\n",
    "\n",
    "# -------------------- Sidebar: Controls + API keys --------------------\n",
    "with st.sidebar:\n",
    "    st.header(\"‚öôÔ∏è Settings\")\n",
    "    mode  = st.selectbox(\"Scoring mode\", [\"hybrid\",\"user\",\"item\"], index=0,\n",
    "                         help=\"hybrid = UserCF ‚®â ItemCF blend; user = neighbors by user; item = neighbors by your watched items\")\n",
    "    top_n = st.slider(\"Top-N\", 5, 50, 12, 1)\n",
    "    ku    = st.slider(\"User-CF neighbors (ku)\", 10, 300, 80, 5, help=\"How many similar users inform your scores\")\n",
    "    ki    = st.slider(\"Item-CF neighbors (ki)\", 20, 400, 140, 10, help=\"How many similar items per watched item\")\n",
    "    alpha = st.slider(\"Hybrid weight Œ± (user ‚Üî item)\", 0.0, 1.0, 0.6, 0.05, help=\"Higher Œ± favors user-based signals\")\n",
    "\n",
    "    st.divider()\n",
    "    st.markdown(\"### Filters\")\n",
    "    include_g = st.multiselect(\"Include any of genres\", options=ALL_GENRES, default=[],\n",
    "                               help=\"Show only items matching at least one of these genres\")\n",
    "    exclude_g = st.multiselect(\"Exclude genres\", options=ALL_GENRES, default=[],\n",
    "                               help=\"Hide any item containing these genres\")\n",
    "\n",
    "    min_year = int(pd.Series([y for y in DF_MOVIES[\"year\"] if y]).min() or 1900)\n",
    "    max_year = int(pd.Series([y for y in DF_MOVIES[\"year\"] if y]).max() or 2025)\n",
    "    yr_min, yr_max = st.slider(\"Year range\", min_year, max_year, value=(1990, max_year),\n",
    "                               help=\"Restrict results to a specific release window\")\n",
    "    min_cnt = st.slider(\"Minimum rating count\", 1, int(ITEM_COUNTS.max()), value=5, step=1,\n",
    "                        help=\"Require at least this many ratings for robustness\")\n",
    "\n",
    "    st.divider()\n",
    "    st.markdown(\"### Re-ranking\")\n",
    "    beta  = st.slider(\"Genre boost (Œ≤)\", 0.0, 1.0, 0.35, 0.05, help=\"Favor items matching your inferred genres\")\n",
    "    recency = st.slider(\"Recency boost\", 0.0, 1.0, 0.30, 0.05, help=\"Prefer newer titles a bit more\")\n",
    "    serend = st.slider(\"Serendipity (exploration)\", 0.0, 1.0, 0.10, 0.05, help=\"Add tiny noise to discover long-tail items\")\n",
    "    lam   = st.slider(\"MMR diversity (Œª)\", 0.1, 0.95, 0.70, 0.05, help=\"Higher=more relevance, lower=more diversity\")\n",
    "\n",
    "    st.divider()\n",
    "    st.markdown(\"### API Keys (optional)\")\n",
    "    tmdb_key_default = os.getenv(\"TMDB_API_KEY\", \"\")\n",
    "    omdb_key_default = os.getenv(\"OMDB_API_KEY\", \"\")\n",
    "    tmdb_key = st.text_input(\"TMDb API Key\", type=\"password\", value=tmdb_key_default,\n",
    "                             help=\"Used for posters. Free key at themoviedb.org\")\n",
    "    omdb_key = st.text_input(\"OMDb API Key\", type=\"password\", value=omdb_key_default,\n",
    "                             help=\"Used for plot/IMDB rating. Get a key at omdbapi.com\")\n",
    "    use_posters = st.toggle(\"Use TMDb posters\", value=bool(tmdb_key))\n",
    "    use_omdb    = st.toggle(\"Use OMDb metadata\", value=bool(omdb_key))\n",
    "    st.caption(f\"Model meta: Recall@10 (LOO) = {B['meta'].get('recall10')}, Threads = {B['meta'].get('threads')}\")\n",
    "\n",
    "# -------------------- Personalization source (silent) --------------------\n",
    "profile_src = \"session\"\n",
    "affinity = session_genre_affinity()\n",
    "if USER_ID_PARAM is not None and USER_ID_PARAM in UID_MAP:\n",
    "    affinity_ds = user_genre_affinity_from_dataset(USER_ID_PARAM)\n",
    "    if affinity_ds:\n",
    "        affinity = affinity_ds\n",
    "        profile_src = f\"user_id={USER_ID_PARAM}\"\n",
    "\n",
    "# Utilities to set or copy a permalink with user_id\n",
    "def set_permalink_user(uid: int):\n",
    "    st.query_params[\"user_id\"] = str(uid)\n",
    "\n",
    "# -------------------- Recommender core --------------------\n",
    "def recommend_scores_for_uid(uidx: int) -> Dict[int,float]:\n",
    "    if mode == \"user\":\n",
    "        return _usercf_scores(uidx, ku)\n",
    "    elif mode == \"item\":\n",
    "        return _itemcf_scores(uidx, ki)\n",
    "    else:\n",
    "        return _hybrid_scores(uidx, ku=ku, ki=ki, alpha=alpha)\n",
    "\n",
    "def recommend_now() -> pd.DataFrame:\n",
    "    # 1) Base ranking\n",
    "    if USER_ID_PARAM is not None and USER_ID_PARAM in UID_MAP:\n",
    "        uidx = UID_MAP[USER_ID_PARAM]\n",
    "        scores = recommend_scores_for_uid(uidx)\n",
    "        ranked = sorted(scores, key=scores.get, reverse=True)\n",
    "    else:\n",
    "        ranked = list(POP_IDX)\n",
    "\n",
    "    # Convert to dict for boosts (top slice for speed)\n",
    "    base_dict = {j: (1.0 - (r/len(ranked))) for r, j in enumerate(ranked[:3000])}\n",
    "\n",
    "    # 2) Boost by inferred genres\n",
    "    boosted = boost_by_genre(base_dict, beta=beta, affinity=affinity)\n",
    "\n",
    "    # 3) Recency & Serendipity\n",
    "    boosted = apply_recency(boosted, recency)\n",
    "    boosted = add_serendipity(boosted, serend)\n",
    "\n",
    "    ranked = sorted(boosted, key=boosted.get, reverse=True)\n",
    "\n",
    "    # 4) Filters\n",
    "    ranked = apply_filters(ranked, include_g, exclude_g, yr_min, yr_max, min_cnt)\n",
    "\n",
    "    # 5) Diversity (MMR)\n",
    "    ranked = mmr_diversify(ranked, top_n=top_n, lam=lam) if ranked else ranked\n",
    "\n",
    "    return _idx_to_frame(ranked[:top_n])\n",
    "\n",
    "# -------------------- Optional: posters & OMDb --------------------\n",
    "@st.cache_data(show_spinner=False, ttl=3600)\n",
    "def tmdb_poster(title: str, year: int | None, key: str):\n",
    "    if not key or not title:\n",
    "        return None\n",
    "    try:\n",
    "        q = {\"api_key\": key, \"query\": title}\n",
    "        if year: q[\"year\"] = year\n",
    "        r = requests.get(\"https://api.themoviedb.org/3/search/movie\", params=q, timeout=8)\n",
    "        r.raise_for_status()\n",
    "        res = r.json().get(\"results\", [])\n",
    "        if not res: return None\n",
    "        path = res[0].get(\"poster_path\")\n",
    "        return f\"https://image.tmdb.org/t/p/w342{path}\" if path else None\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "@st.cache_data(show_spinner=False, ttl=1800)\n",
    "def omdb_meta(title: str, year: int | None, key: str):\n",
    "    if not key or not title:\n",
    "        return {}\n",
    "    try:\n",
    "        q = {\"t\": title, \"apikey\": key}\n",
    "        if year: q[\"y\"] = str(year)\n",
    "        r = requests.get(\"https://www.omdbapi.com/\", params=q, timeout=8)\n",
    "        r.raise_for_status()\n",
    "        j = r.json()\n",
    "        if j.get(\"Response\") != \"True\": return {}\n",
    "        return {\n",
    "            \"imdbRating\": j.get(\"imdbRating\"),\n",
    "            \"Runtime\": j.get(\"Runtime\"),\n",
    "            \"Genre\": j.get(\"Genre\"),\n",
    "            \"Plot\": j.get(\"Plot\"),\n",
    "        }\n",
    "    except Exception:\n",
    "        return {}\n",
    "\n",
    "# -------------------- Main panel: Recommendations --------------------\n",
    "df_rec = recommend_now()\n",
    "st.subheader(\"Recommended for you\")\n",
    "st.caption(f\"Personalization source: **{profile_src}**  ‚Ä¢  Add üëç to refine in-session  ‚Ä¢  Optional: pass ?user_id=<id>\")\n",
    "\n",
    "if USER_ID_PARAM is not None and USER_ID_PARAM in UID_MAP:\n",
    "    col = st.columns([1,1,2,2])[0]\n",
    "    with col:\n",
    "        if st.button(\"Copy permalink for this user\", use_container_width=True):\n",
    "            set_permalink_user(USER_ID_PARAM)\n",
    "            st.success(\"Query parameter set. Share this URL to reproduce personalization.\")\n",
    "\n",
    "if df_rec.empty:\n",
    "    st.warning(\"No results with current filters. Try relaxing constraints.\")\n",
    "else:\n",
    "    show_df = df_rec.copy()\n",
    "    if use_posters:\n",
    "        show_df[\"poster\"] = [tmdb_poster(t, y, tmdb_key) for t, y in zip(show_df[\"title\"], show_df[\"year\"])]\n",
    "    if use_omdb:\n",
    "        metas = [omdb_meta(t, y, omdb_key) for t, y in zip(show_df[\"title\"], show_df[\"year\"])]\n",
    "        show_df[\"imdbRating\"] = [m.get(\"imdbRating\") for m in metas]\n",
    "        show_df[\"Plot\"]       = [m.get(\"Plot\") for m in metas]\n",
    "\n",
    "    st.dataframe(show_df, use_container_width=True, hide_index=True)\n",
    "    st.download_button(\"Download CSV\", show_df.to_csv(index=False).encode(\"utf-8\"),\n",
    "                       file_name=\"recs.csv\", mime=\"text/csv\")\n",
    "\n",
    "# -------------------- Quick feedback (implicit personalization) --------------------\n",
    "st.markdown(\"#### Quick feedback (optional)\")\n",
    "st.caption(\"Click üëç to like a title. This silently tunes your session profile (genre boost).\")\n",
    "cols = st.columns(len(df_rec) if len(df_rec) else 1)\n",
    "for i, row in df_rec.iterrows():\n",
    "    with cols[i % len(cols)]:\n",
    "        if st.button(\"üëç\", key=f\"like_{int(row.movieId)}\"):\n",
    "            st.session_state.session_likes.add(int(row.movieId))\n",
    "            st.rerun()  # modern API\n",
    "\n",
    "# -------------------- Similar movies expander --------------------\n",
    "with st.expander(\"Find similar to a title\"):\n",
    "    options = DF_MOVIES.sort_values(\"title\")[[\"movieId\",\"title\"]]\n",
    "    pick = st.selectbox(\"Choose a title\", options[\"title\"].tolist(), index=0)\n",
    "    mid = int(options.loc[options[\"title\"] == pick, \"movieId\"].iloc[0])\n",
    "    itx = IID_MAP.get(mid)\n",
    "    if itx is not None:\n",
    "        sims = (R_ITEM.T @ R_ITEM[:, itx]).toarray().ravel()\n",
    "        sims[itx] = 0.0\n",
    "        k = st.slider(\"How many similar?\", 5, 30, 12)\n",
    "        neigh = np.argpartition(-sims, k)[:k]\n",
    "        neigh = neigh[np.argsort(-sims[neigh])]\n",
    "        df_sim = _idx_to_frame(neigh.tolist())\n",
    "        if use_posters:\n",
    "            df_sim[\"poster\"] = [tmdb_poster(t, parse_year(t), tmdb_key) for t in df_sim[\"title\"]]\n",
    "        st.dataframe(df_sim, use_container_width=True, hide_index=True)\n",
    "\n",
    "# -------------------- About / Help --------------------\n",
    "st.markdown(\"---\")\n",
    "with st.expander(\"‚ÑπÔ∏è How this works & what the controls do\"):\n",
    "    st.markdown(\"\"\"\n",
    "**Pipeline**\n",
    "- **UserCF**: Finds neighbors with similar rating patterns; recommends what they liked.\n",
    "- **ItemCF**: Finds movies similar to the ones you rated; recommends analogues.\n",
    "- **Hybrid (Œ±)**: Weighted blend of UserCF and ItemCF (Œ±=1 ‚Üí pure UserCF, Œ±=0 ‚Üí pure ItemCF).\n",
    "\n",
    "**Personalization (no prompts)**\n",
    "- If the URL has `?user_id=123` and that user exists in the dataset, we infer a **genre profile** from their ratings and boost matching films.\n",
    "- Without a user id, we start with a **Bayesian-smoothed popularity** list; as you click **üëç**, we update a session genre profile to nudge results.\n",
    "\n",
    "**Filters**\n",
    "- Include / Exclude genres: quick content control.\n",
    "- Year range & Minimum rating count: quality guardrails (avoid ultra-obscure items if you want).\n",
    "\n",
    "**Re-ranking**\n",
    "- **Genre boost (Œ≤)**: multiplies scores by `(1 + Œ≤ * affinity_to_item_genres)`.\n",
    "- **Recency boost**: gently favors newer titles using an exponential age decay.\n",
    "- **Serendipity**: tiny score noise so long-tail titles occasionally surface.\n",
    "- **MMR diversity (Œª)**: trades off relevance vs. similarity to already-picked items to reduce near-duplicates.\n",
    "\n",
    "**APIs**\n",
    "- **TMDb** (posters): paste your key; we call `/search/movie` and build poster URLs (`image.tmdb.org/t/p/w342/...`).\n",
    "- **OMDb** (metadata): paste your key; we call `/?t=<title>&y=<year>` to fetch Plot and IMDb rating.\n",
    "- Keys are used **only in-session** and not saved to disk.\n",
    "\n",
    "**Links**\n",
    "- Share a personalized view by adding `?user_id=<dataset_user_id>` to the app URL (use the ‚ÄúCopy permalink for this user‚Äù button).\n",
    "\"\"\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
